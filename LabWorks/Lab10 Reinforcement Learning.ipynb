{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"P10 Reinforcement Learning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"59Eh5IkOkhsN"},"source":["# MSE MachLe -- Reinforcement Learning\n","**Helmut Grabner, Autum Term, 2020**\n","\n","### Objectives:\n","+ Apply (classical) Q-learning on the (classical) cartpole example\n","+ [Bonus] use other environments!\n","\n","### Questions:\n","+ What is the influence of the different parameters, such as learning rate, exploration rate, discount factor, etc.?\n","+ How many episodes are needed?\n","\n","### Credits\n","+ Hughly inspired by: https://github.com/RJBrooker/Q-learning-demo-Cartpole-V1/blob/master/cartpole.ipynb\n","\n","### Additional Resources\n","+ https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n","+ https://gym.openai.com/docs/"]},{"cell_type":"code","metadata":{"id":"hj96EgEMuxaV"},"source":["from __future__ import absolute_import, division, print_function\n","\n","import base64\n","import imageio\n","import IPython\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL.Image\n","import gym\n","import math\n","from typing import Tuple\n","from sklearn.preprocessing import KBinsDiscretizer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mz9BOSgouxac"},"source":["#if you're using colab you may need to install xvfb to display the environment\n","!apt-get install xvfb\n","!pip install pyvirtualdisplay\n","\n","# Set up a virtual display for rendering OpenAI gym environments.\n","import pyvirtualdisplay\n","display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MVRP6-Fnceuu"},"source":["## Environment\n","\n","Here we will use the well-known CartPole gym-environment. Our agent has to learn how to balance a pole on a car. The agent observes the position and velocity of the car and the angular position and velocity of the pole. In every step, he can move right or left."]},{"cell_type":"code","metadata":{"id":"pYEz-S9gEv2-"},"source":["env_name = 'CartPole-v1'\n","env = gym.make(env_name).env"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hqgAI_Imceu2"},"source":["Lets take a look at the start situation. Observation corresponds to: [position of the car, velocity of the cart, angular position of the pole, velocitiy of the pole]"]},{"cell_type":"code","metadata":{"id":"Aw1sH3scceu3"},"source":["time_step = env.reset()\n","print(time_step)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8JaWxcsceu8"},"source":["If we act and let the car move right, we see the new situation. Additionally, the reward increased by one, because we were able to keep the pole above a given angel for one step."]},{"cell_type":"code","metadata":{"id":"-n6ylzqxceu8"},"source":["action = np.array(1, dtype=np.int32)\n","\n","next_time_step = env.step(action)\n","print('Next time step:')\n","print(next_time_step)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WX1tjKxsuxak"},"source":["## Visualise Enviroment\n","Visualise the eniroment/simulation -- with a **RANDOM** policy (e.g., moving left or right randomly)"]},{"cell_type":"code","metadata":{"id":"-DoVnL7Euxak"},"source":["#monitor\n","from gym import wrappers\n","env = wrappers.Monitor(env, \"./gym-results\", force=True)\n","\n","#reset environment\n","env.reset()\n","for t in range(500):\n","  #choose random action\n","  action = env.action_space.sample()\n","  #apply action, get new state, reward, etc.\n","  observation, reward, done, info = env.step(action)\n","  if done:\n","    print(\"Episode finished after {} timesteps\".format(t+1))\n","    break\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08ZIfFwJuxam"},"source":["import io\n","import base64\n","from IPython.display import HTML\n","\n","video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n","encoded = base64.b64encode(video)\n","HTML(data='''\n","    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",".format(encoded.decode('ascii')))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfl0YkN_uxaw"},"source":["## Q-learning"]},{"cell_type":"markdown","metadata":{"id":"qr-mdcvUuxax"},"source":["Convert Catpoles continues state space into discrete one. "]},{"cell_type":"code","metadata":{"id":"WmxL3wJOuxay"},"source":["n_bins = ( 6 , 12 )\n","lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n","upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n","\n","def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n","    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n","    est.fit([lower_bounds, upper_bounds ])\n","    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dWsxWeX_uxa0"},"source":["Initialise the Q value table with zeros. Interpret the table!"]},{"cell_type":"code","metadata":{"id":"qZKVpoHLuxa1"},"source":["Q_table = np.zeros(n_bins + (env.action_space.n,))\n","Q_table.shape\n","#print (Q_table)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKBPU4kDuxa3"},"source":["Create a polict function,  uses the Q-table to and greedly selecting the highest Q value "]},{"cell_type":"code","metadata":{"id":"OXrqPZQUuxa3"},"source":["def policy( state : tuple ):\n","\n","    ### INSERT YOUR CODE HERE ###\n","\n","\n","    ### END YOUR CODE HERE ###\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0s1-Tf6-uxa5"},"source":["Update function "]},{"cell_type":"code","metadata":{"id":"ILD3mBEHuxa5"},"source":["def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n","    #Temperal diffrence for updating Q-value of state-action pair\n","    \n","    ### INSERT YOUR CODE HERE ###\n","\n","\n","    ### END YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_As5gJhnuxa8"},"source":["Decaying of learning rate and exploration rate"]},{"cell_type":"code","metadata":{"id":"T5VCsmWMuxa8"},"source":["def learning_rate(n : int , min_rate=0.01 ) -> float  :\n","    #Decaying learning rate decay\n","    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n","\n","def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n","    #Decaying exploration rate\n","    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TPJoxGa5uxbB"},"source":["## Training\n","\n","Make yourself familar with the code. Spot policy, environment update, etc. Play around with differnt settings..."]},{"cell_type":"code","metadata":{"id":"pEZUNQH2uxbC"},"source":["n_episodes = 300\n","n_maxtimesteps = 10000\n","timesteps= [];\n","\n","discount_factor = 1\n","\n","for e in range(n_episodes):\n","    \n","    # Siscretize state into buckets\n","    current_state, done = discretizer(*env.reset()), False\n","    \n","    for t in range(n_maxtimesteps):\n","    \n","        # policy action \n","        action = policy(current_state) # exploit\n","        \n","        # insert random action\n","        if np.random.random() < exploration_rate(e) : \n","            action = env.action_space.sample() # explore \n","         \n","        # increment enviroment\n","        obs, reward, done, _ = env.step(action)\n","        new_state = discretizer(*obs)\n","        \n","        # Update Q-Table\n","        lr = learning_rate(e)\n","        learnt_value = new_Q_value(reward , new_state, discount_factor)\n","\n","        old_value = Q_table[current_state][action]\n","        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n","        \n","        current_state = new_state\n","\n","        if done:\n","           print(\"Episode {} finished after {} timesteps\".format(e, t+1))\n","           timesteps.append(t+1)\n","           break\n","        \n","    #close environmnet      \n","    env.close()\n","\n","plt.plot(timesteps);\n","plt.xlabel(\"epidodes\");\n","plt.ylabel(\"timesteps of successfull balancing\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6xgc7--lGMR"},"source":["Check the learnted Q-Table"]},{"cell_type":"code","metadata":{"id":"NHCUkWw5uxbF"},"source":["print (Q_table)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qk27RbiKsQWZ"},"source":["Now, run the learned policy..."]},{"cell_type":"code","metadata":{"id":"jzdG6TuzqpMs"},"source":["env.reset()\n","env = wrappers.Monitor(env, \"./gym-results\", force=True)\n","\n","#reset environment\n","current_state, done = discretizer(*env.reset()), False\n","\n","for t in range(n_maxtimesteps): \n","  # policy action \n","  action = policy(current_state) # exploit\n","    \n","  # increment enviroment\n","  obs, reward, done, _ = env.step(action)\n","  current_state = discretizer(*obs)\n","\n","  env.render()\n","\n","  if done:\n","      print(\"Finished after {} timesteps\".format(t+1))\n","      break\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WstFwgqSlJ5q"},"source":["Render Video for the last episode -- compare to previously video..."]},{"cell_type":"code","metadata":{"id":"FuMO5HlP2R01"},"source":["import io\n","import base64\n","from IPython.display import HTML\n","\n","video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n","encoded = base64.b64encode(video)\n","HTML(data='''\n","    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",".format(encoded.decode('ascii')))"],"execution_count":null,"outputs":[]}]}